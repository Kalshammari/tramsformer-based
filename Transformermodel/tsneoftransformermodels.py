# -*- coding: utf-8 -*-
"""tSNEoftransformermodels.ipynb

Automatically generated by Colaboratory.


"""

#Import The Solar Flare Data Set Files
!wget https://www.dropbox.com/s/uy58al2rwf6yn9u/labels_1540_4classes_icmla_21.pck
!wget https://www.dropbox.com/s/4bt5ugb9rimbrgx/mvts_1540_icmla_21.pck

import numpy as np
import pandas as pd
import networkx as nx
import pickle
from tensorflow import keras
from keras import layers
from sklearn.model_selection import train_test_split
import tensorflow as tf
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from keras.models import Model
from keras.layers import Input, Dense, Dropout
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from keras.losses import SparseCategoricalCrossentropy
from keras.metrics import SparseCategoricalAccuracy
import matplotlib.pyplot as plt
import seaborn as sns
from keras.utils import plot_model
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, confusion_matrix, adjusted_rand_score

#Reading pickle files
def load(file_name):
    with open(file_name, 'rb') as fp:
        obj = pickle.load(fp)
    return obj


Sampled_inputs=load("mvts_1540_icmla_21.pck")

Sampled_labels=load("labels_1540_4classes_icmla_21.pck")

temp=Sampled_inputs[0]
print(temp)
df = pd.DataFrame(temp)
trainData=Sampled_inputs
trainLabel=Sampled_labels
print("trainData.shape: ",trainData.shape)
print("trainLebel.shape: ",trainLabel.shape)

df = pd.DataFrame(trainData[0])
df

print(trainLabel[0])

#standardization/z normalization of the univaraite time series
#-------------------data transform 3D->2D->3D ------------------------------
#Takes 3D array(x,y,z) >> transpose(y,z) >> return (x,z,y)
def GetTransposed2D(arrayFrom):
    toReturn = []
    alen = arrayFrom.shape[0]
    for i in range(0, alen):
        toReturn.append(arrayFrom[i].T)

    return np.array(toReturn)

#Takes 3D array(x,y,z) >> Flatten() >> return (x*y,z)
def Make2D(array3D):
    toReturn = []
    x = array3D.shape[0]
    y = array3D.shape[1]
    for i in range(0, x):
        for j in range(0, y):
            toReturn.append(array3D[i,j])

    return np.array(toReturn)

#Transform instance(92400, 33) into(1540x60x33)
def Get3D_MVTS_from2D(array2D, windowSize):
    arrlen = array2D.shape[0]
    mvts = []
    for i in range(0, arrlen, windowSize):
        mvts.append(array2D[i:i+windowSize])

    return np.array(mvts)




#-------------------data Scaler ------------------------------------------
from sklearn.preprocessing import StandardScaler

#LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized

def GetStandardScaler(data2d):
    scaler = StandardScaler()
    scaler = scaler.fit(data2d)
    return scaler

def GetStandardScaledData(data2d):
    scaler = StandardScaler()
    scaler = scaler.fit(data2d)
    #print(scaler.mean_)
    data_scaled = scaler.transform(data2d)
    return data_scaled

def transform_scale_data(data3d, scaler):
    print("original data shape:", data3d.shape)
    trans = GetTransposed2D(data3d)
    print("transposed data shape:", trans.shape)    #(x, 60, 33)
    data2d = Make2D(trans)
    print("2d data shape:", data2d.shape)
    #  scaler = GetStandardScaler(data2d)
    data_scaled = scaler.transform(data2d)
    mvts_scalled = Get3D_MVTS_from2D(data_scaled, data3d.shape[2])#,60)
    print("mvts data shape:", mvts_scalled.shape)
    transBack = GetTransposed2D(mvts_scalled)
    print("transBack data shape:", transBack.shape)
    return transBack

TORCH_SEED = 0
#building standard scaler on train data X

#---------------Node Label Data Scaling-----------
trans = GetTransposed2D(trainData)
data2d = Make2D(trans)
scaler = GetStandardScaler(data2d)

trainData = transform_scale_data(trainData, scaler)
#trainLabel = trainLabel
unique_y_train, counts_y_train = np.unique(trainLabel, return_counts=True)
num_y_class = unique_y_train.shape[0]
print("X_train shape: ", trainData.shape)
print("y_train shape: ", trainLabel.shape)
#y_train_stats = dict(zip(unique_y_train, counts_y_train))
print("unique_y_train: ", unique_y_train)
print("y_train_counts: ", counts_y_train)
print("num_y_class: ", num_y_class)

df = pd.DataFrame(trainData[0])
df

print(trainData)

#Transposing trainData to shape:(1540, 60, 33)
trainDatatemp=np.empty([1540,60, 33])
n=len(trainData)
for l in range(0, n):
  temp=trainData[l]
  temp=temp.T
  trainDatatemp[l,:,:]=temp


trainData=trainDatatemp
print("Transposing trainData shape: ",trainData.shape)

#Taking the first 25 parameters which are based parameters:(1540, 60, 25)
trainDatat1=np.empty([1540,60, 25])
n=len(trainData)
for l in range(0, n):
  temp=trainData[l,:,0:25]
  trainDatat1[l,:,:]=temp


trainData=trainDatat1
print("Transposing trainData shape: ",trainData.shape)

from tensorflow import keras
from keras import layers
from sklearn.model_selection import train_test_split

def transformer_encoder(inputs, head_size, num_heads, ff_dim):
    # Normalization and Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads
    )(x, x)
    #x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    #x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res

def build_model(
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    #dropout=0,
    #mlp_dropout=0,
):
    n_classes=len(unique_y_train)
    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim)

    x = layers.GlobalAveragePooling1D(data_format="channels_first")(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation="relu")(x)
        #x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(n_classes, activation="softmax")(x)
    return keras.Model(inputs, outputs)

from sklearn.manifold import TSNE
def show_tsne_visualization(model, data,y, epochs):
  """Shows a t-SNE visualization of the model for a given dataset after a given number of epochs."""
  print("count data",len(data))
  # Get the predictions from the model.
  predictions = model.predict(data)

  # Convert the predictions to a NumPy array.
  predictions = np.array(predictions)

  # Perform t-SNE on the predictions.
  tsne_model = TSNE(n_components=2, perplexity=30, random_state=42)
  tsne_results = tsne_model.fit_transform(predictions)
  xe1 = tsne_results[:,0]
  xe2 = tsne_results[:,1]
    #plt.title('t-SNE representation')


  df = pd.DataFrame({'t-SNE dimension 1':xe1, 't-SNE dimension 2':xe2, 'Class':y})
  df = df.sort_values(by=['Class'], ascending=True)

  legend_map = {0: 'X',
                1: 'M',
                2: 'BC',
                3: 'Q'}
  fig = plt.figure(figsize=(11, 11))
  sns.set(font_scale=2)
  ax = sns.scatterplot(x=df['t-SNE dimension 1'], y=df['t-SNE dimension 2'], hue=df['Class'].map(legend_map),
                         palette=['red', 'orange', 'blue', 'green'], legend='full')
  #https://www.tutorialspoint.com/save-the-plots-into-a-pdf-in-matplotlib
  plt.savefig("TSNEMulti.pdf", format="pdf", bbox_inches="tight")
  plt.savefig('tsne_plot4Multi.png')

input_shape = trainData.shape[1:]
# Create the model.
model = build_model( input_shape,
    head_size=256,
    num_heads=4,
    ff_dim=4,
    num_transformer_blocks=10,
    mlp_units=[64])
model.compile(
    loss="sparse_categorical_crossentropy",
    optimizer=keras.optimizers.Adam(learning_rate=1e-4),
    metrics=["sparse_categorical_accuracy"],)
  # Train the model.
model.fit(trainData, trainLabel, epochs=200)

# Show the t-SNE visualization.
show_tsne_visualization(model, trainData, trainLabel,200)
